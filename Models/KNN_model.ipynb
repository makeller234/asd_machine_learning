{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sklearn to prevent version mismatches\n",
    "!pip install sklearn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install joblib. This will be used to save your model. \n",
    "# Restart your kernel after installing \n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Read the CSV and Perform Basic Data Cleaning    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv\n",
    "df = pd.read_csv(\"../Resources/Autism.csv\")\n",
    "\n",
    "# Drop the null columns where all values are null\n",
    "df = df.dropna(axis='columns', how='all')\n",
    "\n",
    "# Drop the null rows\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode strings into binary format\n",
    "df = pd.get_dummies(df)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select ML features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features. Start off with all inputs except 'Case No'\n",
    "X = df.drop(columns=['Class_NO','Class_YES', 'Case No', 'Score'])\n",
    "y = df[['Class_NO','Class_YES']]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection after grid search, and then I can run this cell, altering inputs\n",
    "# NOTE: These are the features I ultimately selected after running ExtraTreeClassifier below Gridsearch\n",
    "Xsel = df[['A5', 'A9', 'A6', 'A3', 'A4', 'A1', 'A2', 'A7', 'A10', 'A8']]\n",
    "y = df[['Class_NO','Class_YES']]\n",
    "print(Xsel.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Train Test Split     \n",
    "*Use 'Class' for the y values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing     \n",
    "*Scale the data using the MinMaxScaler*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#X\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "#y\n",
    "y_scaler = MinMaxScaler().fit(y_train)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set up empty arrays for our training and testing scores\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# For loop \n",
    "for k in range(1, 100, 10):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train_scaled)\n",
    "    train_score = knn.score(X_train_scaled, y_train_scaled)\n",
    "    test_score = knn.score(X_test_scaled, y_test_scaled)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    #print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "plt.plot(range(1, 100, 10), train_scores, marker='o')\n",
    "plt.plot(range(1, 100, 10), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that k: 21 seems to be the best choice for this dataset\n",
    "\n",
    "# Define the model\n",
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "\n",
    "# Fit the model\n",
    "gs = knn.fit(X_train_scaled, y_train_scaled)\n",
    "print('k=21 Test Acc: %.3f' % knn.score(X_test_scaled, y_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {knn.score(X_train_scaled, y_train_scaled)}\")\n",
    "print(f\"Testing Data Score: {knn.score(X_test_scaled, y_test_scaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print off parameters for the KNN model\n",
    "print(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GridSearchCV model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the parameters we want to check\n",
    "param_grid = {'leaf_size': [1, 2, 5, 30, 36], 'n_neighbors': [11, 19, 21, 23, 51], \n",
    "             'p': [1, 2], 'weights': ['uniform', 'distance'],\n",
    "             'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "# Define the grid\n",
    "grid = GridSearchCV(knn, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with GridSearch\n",
    "grid.fit(X_train_scaled,y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch Results\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_) # best TESTING score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at feature importances\n",
    "# KNN doesn't have a feature importances, so we'll use ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train_scaled,y_train_scaled)\n",
    "\n",
    "# For the first go-round, do index=X.columns\n",
    "# Now, what's below is the final feature selection\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = gs.predict(X_test_scaled)\n",
    "print(classification_report(y_test_scaled, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBSERVATIONS\n",
    "\n",
    "Before doing any feature selection, performed pd.get_dummies and kept all inputs except 'Score', 'Class', and 'Case No'. K=21 appears to be where the graph levels off, so using that value, got an accuracy score on the testing data of .925     \n",
    "Training data came back at ~.947  After gridsearch, best parameters were: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 51, 'p': 1, 'weights': 'distance'}       \n",
    "\n",
    "'Best testing score': 0.942652329749104     \n",
    "\n",
    "CLASSIFICATION REPORT:      \n",
    "              \n",
    "                  precision    recall  f1-score   support        \n",
    "\n",
    "           0       0.97      0.92      0.94       190\n",
    "           1       0.85      0.93      0.89        90      \n",
    "\n",
    "       micro avg       0.93      0.93      0.93       280       \n",
    "       macro avg       0.91      0.93      0.92       280       \n",
    "    weighted avg       0.93      0.93      0.93       280         \n",
    "     samples avg       0.93      0.93      0.93       280       \n",
    "\n",
    "\n",
    "--------\n",
    "Second go-round, going to keep questions plus age, sex, family history, and jaundice (ranked in that order on feature selection).  Ethnicity ranks high, but I'm going to leave that out.  I did look at the percentages of autism within the various ethnicities, which can be found in 'Stats_Notebooks', 'Other_Stats'.  For feature selection, I'm only going to focus on questions, age, jaundice, family history, and sex.       \n",
    "\n",
    "So with only questions, age, sex, family history, and jaundice.  Got better results.  Again, graph seems to even out around k=21.  Accuracy improved to 0.954  Training score of ~.9618     \n",
    "Best parameters: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 51, 'p': 2, 'weights': 'distance'}       \n",
    "\n",
    "'Best testing score': 0.965352449223417     \n",
    "\n",
    "CLASSIFICATION REPORT:      \n",
    "\n",
    "                 precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.93      0.96       190\n",
    "           1       0.87      1.00      0.93        90\n",
    "\n",
    "       micro avg       0.95      0.95      0.95       280\n",
    "       macro avg       0.94      0.97      0.95       280\n",
    "    weighted avg       0.96      0.95      0.95       280\n",
    "     samples avg       0.95      0.95      0.95       280     \n",
    "     \n",
    " --------   \n",
    "Third go-round: now going to remove jaundice.  K still seems to level off around 21.  Accuracy of .964, so a full percent higher than with jaundice.  Training score only slightly higher at .965  With gridsearch, again only a small boost:     \n",
    "{'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 19, 'p': 2, 'weights': 'distance'}     \n",
    "\n",
    "'Best testing score': 0.966547192353644     \n",
    "\n",
    "CLASSIFICATION REPORT:  \n",
    "             \n",
    "                 precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.95      0.97       190\n",
    "           1       0.90      1.00      0.95        90\n",
    "\n",
    "        micro avg       0.96      0.96      0.96       280\n",
    "       macro avg       0.95      0.97      0.96       280\n",
    "     weighted avg       0.97      0.96      0.96       280\n",
    "      samples avg       0.96      0.96      0.96       280     \n",
    " -------     \n",
    "\n",
    "\n",
    "Fourth go-round: removing family history now.  Again, k levels off around 21.  No change in testing accuracy, still at .964, the same number with family history.  Training data made a 'significant' jump to ~.976  After gridsearch:     \n",
    "{'algorithm': 'ball_tree', 'leaf_size': 30, 'n_neighbors': 19, 'p': 2, 'weights': 'distance'}     \n",
    "\n",
    "'Best testing score': 0.973715651135006 -- jumped from .964, so not bad.  Interesting to note that the algorithm changed to ball_tree, and leaf_size to 30. Neighbors also changed to 19 (from 51) for the last two.       \n",
    "\n",
    "CLASSIFICATION REPORT:     \n",
    "              \n",
    "                 precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.95      0.97       190\n",
    "           1       0.91      0.99      0.95        90\n",
    "\n",
    "       micro avg       0.96      0.96      0.96       280\n",
    "       macro avg       0.95      0.97      0.96       280\n",
    "    weighted avg       0.97      0.96      0.96       280\n",
    "     samples avg       0.96      0.96      0.96       280\n",
    "\n",
    "---------\n",
    "Fifth go-round: removing sex. K at 21, testing accuracy at .968 so not a huge difference from the 3rd and 4th go-rounds. But training data jumped to ~.986 which again is another full percent from when sex was an included input.  After gridsearch:  {'algorithm': 'brute', 'leaf_size': 1, 'n_neighbors': 19, 'p': 2, 'weights': 'distance'}     \n",
    "\n",
    "'Beste testing score': 0.986857825567503 --testing score jumped roughly 2% from before the hyperparameter tuning.  Algorithm changed to 'brute', which is a first.  Notice weights has never changed.      \n",
    "\n",
    "CLASSIFICATION REPORT:     \n",
    "             \n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.95      0.98       190\n",
    "           1       0.91      1.00      0.95        90\n",
    "\n",
    "       micro avg       0.97      0.97      0.97       280\n",
    "       macro avg       0.95      0.98      0.96       280\n",
    "    weighted avg       0.97      0.97      0.97       280\n",
    "     samples avg       0.97      0.97      0.97       280\n",
    "--------     \n",
    "Final go-round, removing age.  K again at 21, testing accuracy at .979 which is the highest of all tests.  Training data at ~.981  After gridsearch:  {'algorithm': 'auto', 'leaf_size': 5, 'n_neighbors': 21, 'p': 2, 'weights': 'distance'}     \n",
    "\n",
    "'Best testing score': 0.986857825567503 --same testing score with and without age. Change in algorithm, leaf size, neighbors.     \n",
    "\n",
    "CLASSIFICATION REPORT:       \n",
    "                   \n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.98      0.98       190\n",
    "           1       0.97      0.97      0.97        90\n",
    "\n",
    "      micro avg       0.98      0.98      0.98       280\n",
    "      macro avg       0.98      0.98      0.98       280\n",
    "    weighted avg       0.98      0.98      0.98       280\n",
    "    samples avg       0.98      0.98      0.98       280\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations on the classification reports above\n",
    "DEFINITIONS:     \n",
    "<b>precision</b> is the ability of the classifier not to label as positive a sample that is negative     \n",
    "<b>recall</b> is the ability of the classifier to find all the positive samples     \n",
    "<b>F1 score</b> also known as balanced F-score or F-measure.  It's the weighted average of the precision and recall scores, with a best score of 1, worst of 0.     \n",
    "      \n",
    "*Note that in binary classification, recall of the positive class is also known as “sensitivity”; recall of the negative class is “specificity”.*\n",
    "\n",
    "NOTE: I don't fully understand the various averages.  Here's what I have so far:     \n",
    "<b>macro average</b> (averaging the unweighted mean per label);     \n",
    "<b>weighted average</b> (averaging the support-weighted mean per label);     \n",
    "<b>sample average</b> (only for multilabel classification);     \n",
    "<b>micro average</b> (averaging the total true positives, false negatives and false positives) is only shown for multi-label or multi-class with a subset of classes, because it corresponds to accuracy otherwise.        \n",
    "     \n",
    "---------     \n",
    "### <u>F1 score</u> ###       \n",
    "The F1 score had the smallest variance (1%) between label '0' and '1' of all five input tweaks.  The highest variance came before any feature selection, where I only removed 'Case No', 'Class', and 'Score'.  The variance was 5%.  All other input changes resulted in a variance of 2-3%. Also, the F1 Scores were the highest for the '0' input.  Class 'NO' (meaning no autism) has 190 instances versus Class 'YES', so perhaps that explains why the F1 Score is higher for the '0' input across the board.\n",
    "\n",
    "### <u>Precision</u> ### \n",
    "Precision is the ability to accurately label true negatives.  Precision was higher across the board for a classification of No (=0), which makes sense since that sample size is double the yes pool.  The precision for the 'No' classification was pretty steady, varying between .97 and 1, whereas the precision for the 'Yes' classification ranged from .85 to .97.      \n",
    "\n",
    "The precision of the '1' label didn't improve until the last feature selection tweak.  It jumped from 0.91 to .97 in the last feature selection, where we removed all except for the questions.  Can we draw any conclusions from this?  Can we safely assume that the discrepency between labels is due to sample size? I'm unsure.     \n",
    "\n",
    "### <u>Recall</u> ###\n",
    "Precision and recall are related, recall being the ability to accurately label true positives.  So for recall, the 'yes' classification scored higher for all alterations to the feature selection.  So the model can more accurately predict when a person DOES have autism.   \n",
    "\n",
    "### <u>Averages</u> ###\n",
    "Doing gridsearch, the best weights parameter came back as 'distance.' Does this have any relation to the averages in the classification report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
